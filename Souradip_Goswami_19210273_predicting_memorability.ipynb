{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning to predict Video Memorability Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary packages\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import mlxtend\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPrind in ./anaconda3/lib/python3.7/site-packages (2.11.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPrind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/souradipgoswami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Captions Dataset and Ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the captions into a data frame from a text file\n",
    "def read_caps(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "# load the captions\n",
    "caption_path = '/Users/souradipgoswami/Desktop/dev-set_video-captions.txt'\n",
    "df_caption=read_caps(caption_path)\n",
    "#load ground truth\n",
    "labels=pd.read_csv('/Users/souradipgoswami/Desktop/ground-truth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to remove stopwords and punctuations from the Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Stopwords: 179\n"
     ]
    }
   ],
   "source": [
    "#loading the nltk stopwords of English\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(f'Length of Stopwords: {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting word occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "pbar = pyprind.ProgBar(len(df_caption['caption']), title='Counting word occurrences')\n",
    "for i, caption in enumerate(df_caption['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in caption]).lower()\n",
    "    #removing stopwords\n",
    "    rmv_stopwords= ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    df_caption.loc[i,'caption'] = rmv_stopwords #updating the original captions \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting word to bag of captions using vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",max_features=3112) \n",
    "captions_bag = vectorizer.fit_transform(df_caption.caption).toarray()\n",
    "type(captions_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 3112)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_bag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing train and validation data and applying the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = captions_bag\n",
    "y = labels[['short-term_memorability','long-term_memorability']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and validation set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the Random Forest Regressor\n",
    "captions_rf = RandomForestRegressor(n_estimators=200,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=200, n_jobs=None, oob_score=False,\n",
       "                      random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the regressor\n",
    "captions_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the validation set\n",
    "prediction1 = captions_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the function for generating the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Spearman coefficient scores\n",
    "def Get_score(Y_pred,Y_true):\n",
    "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
    "    Y_pred = np.squeeze(Y_pred)\n",
    "    Y_true = np.squeeze(Y_true)\n",
    "    if Y_pred.shape != Y_true.shape:\n",
    "        print('Input shapes don\\'t match!')\n",
    "    else:\n",
    "        if len(Y_pred.shape) == 1:\n",
    "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
    "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
    "            print('The Spearman\\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])\n",
    "        else:\n",
    "            for ii in range(Y_pred.shape[1]):\n",
    "                Get_score(Y_pred[:,ii],Y_true[:,ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.414\n",
      "The Spearman's correlation coefficient is: 0.179\n"
     ]
    }
   ],
   "source": [
    "#generating the score\n",
    "Get_score(prediction1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2: Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the captions and ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_caps1(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "\n",
    "\n",
    "# load the captions\n",
    "caption_path1 = '/Users/souradipgoswami/Desktop/dev-set_video-captions.txt'\n",
    "df_caption1=read_caps1(caption_path1)\n",
    "#load ground truth\n",
    "labels1=pd.read_csv('/Users/souradipgoswami/Desktop/ground-truth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the datasets to remove punctuations and apply one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets count the words and remove punctuations\n",
    "counts = Counter()\n",
    "for i, caption in enumerate(df_caption1['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in caption]).lower()\n",
    "    df_caption1.loc[i,'caption'] = text\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_token = len(counts) # create length of token\n",
    "tokenizer = Tokenizer(num_words=len_token) # use tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(df_caption1.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## Convert the integers into binary 0 and 1 format\n",
    "one_hot_enc = tokenizer.texts_to_matrix(list(df_caption1.caption.values),mode='binary')\n",
    "print(one_hot_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the predictor variables and selecting the train and validation sets and applying the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the predictor and response variables\n",
    "predictor1 = labels1[['short-term_memorability','long-term_memorability']].values\n",
    "Y = predictor1\n",
    "X = one_hot_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcreating the training and validation data\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 1200 samples\n",
      "Epoch 1/30\n",
      "4800/4800 [==============================] - 1s 274us/step - loss: 0.0574 - accuracy: 0.6275 - val_loss: 0.0246 - val_accuracy: 0.7100\n",
      "Epoch 2/30\n",
      "4800/4800 [==============================] - 1s 152us/step - loss: 0.0348 - accuracy: 0.6281 - val_loss: 0.0196 - val_accuracy: 0.7100\n",
      "Epoch 3/30\n",
      "4800/4800 [==============================] - 1s 157us/step - loss: 0.0293 - accuracy: 0.6385 - val_loss: 0.0173 - val_accuracy: 0.7100\n",
      "Epoch 4/30\n",
      "4800/4800 [==============================] - 1s 157us/step - loss: 0.0253 - accuracy: 0.6662 - val_loss: 0.0161 - val_accuracy: 0.7100\n",
      "Epoch 5/30\n",
      "4800/4800 [==============================] - 1s 254us/step - loss: 0.0229 - accuracy: 0.6635 - val_loss: 0.0153 - val_accuracy: 0.7100\n",
      "Epoch 6/30\n",
      "4800/4800 [==============================] - 1s 169us/step - loss: 0.0217 - accuracy: 0.6742 - val_loss: 0.0148 - val_accuracy: 0.7100\n",
      "Epoch 7/30\n",
      "4800/4800 [==============================] - 1s 158us/step - loss: 0.0200 - accuracy: 0.6771 - val_loss: 0.0145 - val_accuracy: 0.7100\n",
      "Epoch 8/30\n",
      "4800/4800 [==============================] - 1s 156us/step - loss: 0.0191 - accuracy: 0.6848 - val_loss: 0.0143 - val_accuracy: 0.7100\n",
      "Epoch 9/30\n",
      "4800/4800 [==============================] - 1s 157us/step - loss: 0.0186 - accuracy: 0.6848 - val_loss: 0.0142 - val_accuracy: 0.7100\n",
      "Epoch 10/30\n",
      "4800/4800 [==============================] - 1s 160us/step - loss: 0.0175 - accuracy: 0.6896 - val_loss: 0.0141 - val_accuracy: 0.7100\n",
      "Epoch 11/30\n",
      "4800/4800 [==============================] - 1s 162us/step - loss: 0.0168 - accuracy: 0.6894 - val_loss: 0.0140 - val_accuracy: 0.7100\n",
      "Epoch 12/30\n",
      "4800/4800 [==============================] - 1s 155us/step - loss: 0.0165 - accuracy: 0.6906 - val_loss: 0.0140 - val_accuracy: 0.7100\n",
      "Epoch 13/30\n",
      "4800/4800 [==============================] - 1s 156us/step - loss: 0.0160 - accuracy: 0.6965 - val_loss: 0.0140 - val_accuracy: 0.7100\n",
      "Epoch 14/30\n",
      "4800/4800 [==============================] - 1s 157us/step - loss: 0.0157 - accuracy: 0.6967 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 15/30\n",
      "4800/4800 [==============================] - 1s 222us/step - loss: 0.0153 - accuracy: 0.7017 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 16/30\n",
      "4800/4800 [==============================] - 1s 164us/step - loss: 0.0150 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 17/30\n",
      "4800/4800 [==============================] - 1s 152us/step - loss: 0.0147 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 18/30\n",
      "4800/4800 [==============================] - 1s 153us/step - loss: 0.0143 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 19/30\n",
      "4800/4800 [==============================] - 1s 163us/step - loss: 0.0143 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 20/30\n",
      "4800/4800 [==============================] - 1s 226us/step - loss: 0.0141 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 21/30\n",
      "4800/4800 [==============================] - 1s 170us/step - loss: 0.0140 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 22/30\n",
      "4800/4800 [==============================] - 1s 190us/step - loss: 0.0139 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 23/30\n",
      "4800/4800 [==============================] - 1s 168us/step - loss: 0.0138 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 24/30\n",
      "4800/4800 [==============================] - 1s 176us/step - loss: 0.0138 - accuracy: 0.7033 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 25/30\n",
      "4800/4800 [==============================] - 1s 174us/step - loss: 0.0138 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 26/30\n",
      "4800/4800 [==============================] - 1s 160us/step - loss: 0.0137 - accuracy: 0.7035 - val_loss: 0.0139 - val_accuracy: 0.7100\n",
      "Epoch 27/30\n",
      "4800/4800 [==============================] - 1s 179us/step - loss: 0.0136 - accuracy: 0.7035 - val_loss: 0.0138 - val_accuracy: 0.7100\n",
      "Epoch 28/30\n",
      "4800/4800 [==============================] - 1s 167us/step - loss: 0.0136 - accuracy: 0.7035 - val_loss: 0.0138 - val_accuracy: 0.7100\n",
      "Epoch 29/30\n",
      "4800/4800 [==============================] - 1s 157us/step - loss: 0.0136 - accuracy: 0.7033 - val_loss: 0.0138 - val_accuracy: 0.7100\n",
      "Epoch 30/30\n",
      "4800/4800 [==============================] - 1s 180us/step - loss: 0.0136 - accuracy: 0.7035 - val_loss: 0.0138 - val_accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "#defining and fitting the model\n",
    "model = Sequential()\n",
    "#adding drop out to prevent overfitting\n",
    "model.add(layers.Dropout(0.8, input_shape=(len_token,)))\n",
    "# two layers of 20 neurons each with selu activation function\n",
    "model.add(layers.Dense(20,activation='selu',kernel_regularizer=regularizers.l2(0.001), kernel_initializer='zeros'))\n",
    "model.add(layers.Dense(20,activation='selu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.8))\n",
    "#final layer with 2 neurons and activation function as sigmoid\n",
    "model.add(layers.Dense(2,activation='sigmoid'))\n",
    "#using adamax as optimizer and Mean squared error as loss function\n",
    "model.compile(optimizer='adamax',loss='mse',metrics=['accuracy'])\n",
    "#fitting the model with 30 epochs\n",
    "_model = model.fit(X_train1,Y_train1,epochs=30, validation_data=(X_test1,Y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87019867, 0.7860913 ],\n",
       "       [0.85349554, 0.77410007],\n",
       "       [0.86272764, 0.78063405],\n",
       "       ...,\n",
       "       [0.87508655, 0.78975636],\n",
       "       [0.86597896, 0.7829913 ],\n",
       "       [0.8611691 , 0.77951384]], dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting using 1200 validation samples\n",
    "prediction2 = model.predict(X_test1)\n",
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.401\n",
      "The Spearman's correlation coefficient is: 0.211\n"
     ]
    }
   ],
   "source": [
    "#generating the scores\n",
    "Get_score(prediction2, Y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the captions and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_caps2(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "\n",
    "\n",
    "# load the captions\n",
    "caption_path2 = '/Users/souradipgoswami/Desktop/dev-set_video-captions.txt'\n",
    "df_caption2=read_caps1(caption_path2)\n",
    "#load ground truth\n",
    "labels2=pd.read_csv('/Users/souradipgoswami/Desktop/ground-truth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data to remove punctuations and apply sequence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(list(df_caption2.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set max length of all dimensions to 50\n",
    "max_length=50\n",
    "X_seq = np.zeros((len(sequences),max_length))\n",
    "for i in range(len(sequences)):\n",
    "    n = len(sequences[i])\n",
    "    if n==0:\n",
    "        print(i)\n",
    "    else:\n",
    "        X_seq[i,-n:] = sequences[i]\n",
    "X_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the predictor variables, training and validation sets and applying the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor2 = labels2[['short-term_memorability','long-term_memorability']].values\n",
    "Y = predictor2\n",
    "X = X_seq\n",
    "## Train-Validation Split\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/souradipgoswami/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(input_dim=5191, output_dim=20, input_length=50, embeddings_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 1200 samples\n",
      "Epoch 1/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0844 - accuracy: 0.5677 - val_loss: 0.0524 - val_accuracy: 0.7100\n",
      "Epoch 2/30\n",
      "4800/4800 [==============================] - 14s 3ms/step - loss: 0.0522 - accuracy: 0.6179 - val_loss: 0.0372 - val_accuracy: 0.7100\n",
      "Epoch 3/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0384 - accuracy: 0.6502 - val_loss: 0.0276 - val_accuracy: 0.7100\n",
      "Epoch 4/30\n",
      "4800/4800 [==============================] - 16s 3ms/step - loss: 0.0299 - accuracy: 0.6677 - val_loss: 0.0226 - val_accuracy: 0.7100\n",
      "Epoch 5/30\n",
      "4800/4800 [==============================] - 17s 3ms/step - loss: 0.0249 - accuracy: 0.6804 - val_loss: 0.0189 - val_accuracy: 0.7100\n",
      "Epoch 6/30\n",
      "4800/4800 [==============================] - 16s 3ms/step - loss: 0.0216 - accuracy: 0.6867 - val_loss: 0.0170 - val_accuracy: 0.7100\n",
      "Epoch 7/30\n",
      "4800/4800 [==============================] - 17s 3ms/step - loss: 0.0187 - accuracy: 0.6908 - val_loss: 0.0159 - val_accuracy: 0.7100\n",
      "Epoch 8/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0171 - accuracy: 0.6929 - val_loss: 0.0149 - val_accuracy: 0.7100\n",
      "Epoch 9/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0161 - accuracy: 0.6963 - val_loss: 0.0146 - val_accuracy: 0.7100\n",
      "Epoch 10/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0154 - accuracy: 0.6983 - val_loss: 0.0145 - val_accuracy: 0.7100\n",
      "Epoch 11/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0145 - accuracy: 0.7002 - val_loss: 0.0144 - val_accuracy: 0.7100\n",
      "Epoch 12/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0144 - accuracy: 0.7021 - val_loss: 0.0147 - val_accuracy: 0.7100\n",
      "Epoch 13/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0139 - accuracy: 0.7038 - val_loss: 0.0143 - val_accuracy: 0.7100\n",
      "Epoch 14/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0135 - accuracy: 0.7038 - val_loss: 0.0144 - val_accuracy: 0.7100\n",
      "Epoch 15/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0132 - accuracy: 0.7031 - val_loss: 0.0145 - val_accuracy: 0.7100\n",
      "Epoch 16/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0130 - accuracy: 0.7031 - val_loss: 0.0145 - val_accuracy: 0.7100\n",
      "Epoch 17/30\n",
      "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0127 - accuracy: 0.7035 - val_loss: 0.0145 - val_accuracy: 0.7100\n",
      "Epoch 18/30\n",
      "4800/4800 [==============================] - 17s 4ms/step - loss: 0.0125 - accuracy: 0.7035 - val_loss: 0.0150 - val_accuracy: 0.7100\n",
      "Epoch 19/30\n",
      "4800/4800 [==============================] - 17s 4ms/step - loss: 0.0120 - accuracy: 0.7035 - val_loss: 0.0148 - val_accuracy: 0.7100\n",
      "Epoch 20/30\n",
      "4800/4800 [==============================] - 18s 4ms/step - loss: 0.0118 - accuracy: 0.7035 - val_loss: 0.0145 - val_accuracy: 0.7100\n",
      "Epoch 21/30\n",
      "4800/4800 [==============================] - 18s 4ms/step - loss: 0.0117 - accuracy: 0.7033 - val_loss: 0.0150 - val_accuracy: 0.7100\n",
      "Epoch 22/30\n",
      "4800/4800 [==============================] - 24s 5ms/step - loss: 0.0112 - accuracy: 0.7033 - val_loss: 0.0151 - val_accuracy: 0.7100\n",
      "Epoch 23/30\n",
      "4800/4800 [==============================] - 18s 4ms/step - loss: 0.0112 - accuracy: 0.7035 - val_loss: 0.0146 - val_accuracy: 0.7100\n",
      "Epoch 24/30\n",
      "4800/4800 [==============================] - 21s 4ms/step - loss: 0.0111 - accuracy: 0.7040 - val_loss: 0.0152 - val_accuracy: 0.7100\n",
      "Epoch 25/30\n",
      "4800/4800 [==============================] - 17s 4ms/step - loss: 0.0109 - accuracy: 0.7044 - val_loss: 0.0167 - val_accuracy: 0.7100\n",
      "Epoch 26/30\n",
      "4800/4800 [==============================] - 16s 3ms/step - loss: 0.0107 - accuracy: 0.7033 - val_loss: 0.0146 - val_accuracy: 0.7100\n",
      "Epoch 27/30\n",
      "4800/4800 [==============================] - 16s 3ms/step - loss: 0.0105 - accuracy: 0.7044 - val_loss: 0.0151 - val_accuracy: 0.7100\n",
      "Epoch 28/30\n",
      "4800/4800 [==============================] - 16s 3ms/step - loss: 0.0104 - accuracy: 0.7046 - val_loss: 0.0161 - val_accuracy: 0.7100\n",
      "Epoch 29/30\n",
      "4800/4800 [==============================] - 16s 3ms/step - loss: 0.0105 - accuracy: 0.7035 - val_loss: 0.0158 - val_accuracy: 0.7100\n",
      "Epoch 30/30\n",
      "4800/4800 [==============================] - 17s 3ms/step - loss: 0.0102 - accuracy: 0.7052 - val_loss: 0.0169 - val_accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "modelRNN=Sequential() # Create Sequential NN model\n",
    "\n",
    "## add Embedding layer for RNN to map our data into a format suitable for LSTM layer\n",
    "modelRNN.add(layers.Embedding(input_dim=5191, output_dim=20, input_length=50, init='uniform'))\n",
    "\n",
    "## add LSTM layer for some hidden layer and memory into the network\n",
    "modelRNN.add(layers.LSTM(200, activation='selu', recurrent_initializer='uniform', kernel_initializer='zeros', kernel_regularizer=regularizers.l2(0.001)))\n",
    "modelRNN.add(layers.Dropout(0.8))\n",
    "\n",
    "modelRNN.add(layers.Dense(50, activation='selu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "modelRNN.add(layers.Dropout(0.8))\n",
    "\n",
    "## Output layer of 2 neurons for each score with sigmoid activation \n",
    "modelRNN.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "## Compile the model \n",
    "# Optimizer Adamax\n",
    "modelRNN.compile(optimizer='adamax',loss='mse',metrics=['accuracy'])\n",
    "_modelResult = modelRNN.fit(X_train2,Y_train2,epochs=30, validation_data=(X_test2,Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the scores using 1200 samples\n",
    "prediction3 = modelRNN.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.369\n",
      "The Spearman's correlation coefficient is: 0.189\n"
     ]
    }
   ],
   "source": [
    "#generating the Spearman correlation coefficient\n",
    "Get_score(prediction3, Y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model using Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding up the model prediction and averaging them\n",
    "prediction_test1 = np.add(prediction1,prediction2)\n",
    "prediction_test = np.add(prediction_test1,prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicitng the scores using simple average\n",
    "prediction_testfinal = np.divide(prediction_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.450\n",
      "The Spearman's correlation coefficient is: 0.213\n"
     ]
    }
   ],
   "source": [
    "#generating the spearman correlation coefficient\n",
    "Get_score(prediction_testfinal, Y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simple Average ensemble suggests that it performs better than the individual models i.e. Random Forest regressor, MLP and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look into the Stacking ensemble using Support Vector Regressor and Random Forest Regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the caption and ground truth\n",
    "def read_caps(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "# load the captions\n",
    "caption_path = '/Users/souradipgoswami/Desktop/dev-set_video-captions.txt'\n",
    "df_caption=read_caps(caption_path)\n",
    "#load ground truth\n",
    "labels=pd.read_csv('/Users/souradipgoswami/Desktop/ground-truth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to remove stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Stopwords: 179\n"
     ]
    }
   ],
   "source": [
    "#loading the nltk stopwords of English\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(f'Length of Stopwords: {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting word occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "pbar = pyprind.ProgBar(len(df_caption['caption']), title='Counting word occurrences')\n",
    "for i, caption in enumerate(df_caption['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in caption]).lower()\n",
    "    #removing stopwords\n",
    "    rmv_stopwords= ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    df_caption.loc[i,'caption'] = rmv_stopwords #updating the original captions \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating the caption bag using vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",max_features=3112) \n",
    "captions_bag = vectorizer.fit_transform(df_caption.caption).toarray()\n",
    "type(captions_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = captions_bag\n",
    "y_short = labels[['short-term_memorability']].values.ravel()\n",
    "y_long = labels[['long-term_memorability']].values.ravel()\n",
    "# Splitting the dataset into the Training set and Test set for short term scores\n",
    "X_train,X_test,y_shorttrain,y_shorttest = train_test_split(X,y_short,test_size=0.2,random_state=42)\n",
    "# Splitting the dataset into the Training set and Test set for long term scores\n",
    "X_train,X_test,y_longtrain,y_longtest = train_test_split(X,y_long,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.419\n"
     ]
    }
   ],
   "source": [
    "#creating a simple support vector regression model\n",
    "svr = SVR(kernel='rbf')\n",
    "#fitting the model for short-term memorability\n",
    "svr.fit(X_train, y_shorttrain)\n",
    "#predicting the short-term scores\n",
    "pred = svr.predict(X_test)\n",
    "#generating the spearman's correlation coefficient for short videos\n",
    "Get_score(pred, y_shorttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.179\n"
     ]
    }
   ],
   "source": [
    "#fitting the model for long-term memorability\n",
    "svr.fit(X_train, y_longtrain)\n",
    "#predicting the long-term scores\n",
    "pred = svr.predict(X_test)\n",
    "#generating the spearman's correlation coefficient for short videos\n",
    "Get_score(pred, y_longtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble Model using Random Forest and Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_caps(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "# load the captions\n",
    "caption_path = '/Users/souradipgoswami/Desktop/dev-set_video-captions.txt'\n",
    "df_caption=read_caps(caption_path)\n",
    "#load ground truth\n",
    "labels=pd.read_csv('/Users/souradipgoswami/Desktop/ground-truth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to remove the stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Stopwords: 179\n"
     ]
    }
   ],
   "source": [
    "#loading the nltk stopwords of English\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(f'Length of Stopwords: {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting word occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "pbar = pyprind.ProgBar(len(df_caption['caption']), title='Counting word occurrences')\n",
    "for i, caption in enumerate(df_caption['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in caption]).lower()\n",
    "    #removing stopwords\n",
    "    rmv_stopwords= ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    df_caption.loc[i,'caption'] = rmv_stopwords #updating the original captions \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating the caption bag using vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",max_features=3112) \n",
    "captions_bag = vectorizer.fit_transform(df_caption.caption).toarray()\n",
    "type(captions_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = captions_bag\n",
    "y_short1 = labels[['short-term_memorability']].values.ravel()\n",
    "y_long1 = labels[['long-term_memorability']].values.ravel()\n",
    "# Splitting the dataset into the Training set and Test set for short term scores\n",
    "X_trainstack,X_teststack,y_shorttrain1,y_shorttest1 = train_test_split(X_stack,y_short1,test_size=0.2,random_state=42)\n",
    "# Splitting the dataset into the Training set and Test set for long term scores\n",
    "X_trainstack,X_teststack,y_longtrain1,y_longtest1 = train_test_split(X_stack,y_long1,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta regressor model\n",
    "gradboost1 = ensemble.GradientBoostingRegressor(n_estimators=200,learning_rate=0.01)\n",
    "#Base models\n",
    "rf1= RandomForestRegressor()\n",
    "svr1 = SVR(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the stackingcv regressor model\n",
    "stack = StackingCVRegressor(regressors=(svr1, rf1,gradboost1),\n",
    "                            meta_regressor=gradboost1, cv=10,\n",
    "                            use_features_in_secondary=True,\n",
    "                            store_train_meta_features=True,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model and predicting the short-term score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.452\n"
     ]
    }
   ],
   "source": [
    "stack.fit(X_trainstack, y_shorttrain1)\n",
    "pred = stack.predict(X_teststack)\n",
    "#generating the Spearman's correlation coefficient\n",
    "Get_score(pred, y_shorttest1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model and predicting the long-term score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.180\n"
     ]
    }
   ],
   "source": [
    "stack.fit(X_trainstack, y_longtrain1)\n",
    "pred = stack.predict(X_teststack)\n",
    "#generating the Spearman's correlation coefficient\n",
    "Get_score(pred, y_longtest1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Code for Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Models with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the captions into a data frame from a text file\n",
    "def read_caps(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "# load the captions\n",
    "caption_path = '/Users/souradipgoswami/Desktop/test-set-1_video-captions.txt'\n",
    "df_caption=read_caps(caption_path)\n",
    "#load ground truth\n",
    "labels=pd.read_csv('/Users/souradipgoswami/Desktop/ground_truth_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Stopwords: 179\n"
     ]
    }
   ],
   "source": [
    "#loading the nltk stopwords of English\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(f'Length of Stopwords: {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting word occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "pbar = pyprind.ProgBar(len(df_caption['caption']), title='Counting word occurrences')\n",
    "for i, caption in enumerate(df_caption['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in caption]).lower()\n",
    "    #removing stopwords\n",
    "    rmv_stopwords= ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    df_caption.loc[i,'caption'] = rmv_stopwords #updating the original captions \n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting word to bag of captions using vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "captions_bag = vectorizer.fit_transform(df_caption.caption).toarray()\n",
    "type(captions_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1=captions_rf.predict(captions_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_caps1(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "\n",
    "\n",
    "# load the captions\n",
    "caption_path1 = '/Users/souradipgoswami/Desktop/test-set-1_video-captions.txt'\n",
    "df_caption1=read_caps1(caption_path1)\n",
    "#load ground truth\n",
    "labels1=pd.read_csv('/Users/souradipgoswami/Desktop/ground_truth_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets count the words and remove punctuations\n",
    "counts = Counter()\n",
    "for i, caption in enumerate(df_caption1['caption']):\n",
    "    # replace punctuations with space\n",
    "    # convert words to lower case \n",
    "    text = ''.join([c if c not in punctuation else ' ' for c in caption]).lower()\n",
    "    df_caption1.loc[i,'caption'] = text\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_token = 5191 # create length of token\n",
    "tokenizer = Tokenizer(num_words=len_token) # use tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(df_caption1.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## Convert the integers into binary 0 and 1 format\n",
    "one_hot_enc = tokenizer.texts_to_matrix(list(df_caption1.caption.values),mode='binary')\n",
    "print(one_hot_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8489255 , 0.7709423 ],\n",
       "       [0.8933321 , 0.8041558 ],\n",
       "       [0.8798032 , 0.7933656 ],\n",
       "       ...,\n",
       "       [0.87109864, 0.7867634 ],\n",
       "       [0.8692729 , 0.7854035 ],\n",
       "       [0.8735341 , 0.78858244]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred2=model.predict(one_hot_enc)\n",
    "test_pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_caps2(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    video = []\n",
    "    caption = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            video.append(pairs[0])\n",
    "            caption.append(pairs[1])\n",
    "        df['video']=video\n",
    "        df['caption']=caption\n",
    "    return df\n",
    "\n",
    "\n",
    "# load the captions\n",
    "caption_path2 = '/Users/souradipgoswami/Desktop/test-set-1_video-captions.txt'\n",
    "df_caption2=read_caps1(caption_path2)\n",
    "#load ground truth\n",
    "labels2=pd.read_csv('/Users/souradipgoswami/Desktop/ground_truth_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(list(df_caption2.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set max length of all dimensions to 50\n",
    "max_length=50\n",
    "X_seq = np.zeros((len(sequences),max_length))\n",
    "for i in range(len(sequences)):\n",
    "    n = len(sequences[i])\n",
    "    if n==0:\n",
    "        print(i)\n",
    "    else:\n",
    "        X_seq[i,-n:] = sequences[i]\n",
    "X_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9414482 , 0.92183447],\n",
       "       [0.932033  , 0.9053104 ],\n",
       "       [0.9579474 , 0.94750667],\n",
       "       ...,\n",
       "       [0.88413733, 0.8230193 ],\n",
       "       [0.84103835, 0.7474295 ],\n",
       "       [0.8640305 , 0.78808594]], dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred3=modelRNN.predict(X_seq)\n",
    "test_pred3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding up the model prediction and averaging them\n",
    "prediction_sa = np.add(test_pred1,test_pred2)\n",
    "prediction_safinal = np.add(prediction_sa,test_pred3)\n",
    "prediction_finalsa = np.divide(prediction_safinal, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>short-term_memorability</th>\n",
       "      <th>nb_short-term_annotations</th>\n",
       "      <th>long-term_memorability</th>\n",
       "      <th>nb_long-term_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7494</td>\n",
       "      <td>0.884959</td>\n",
       "      <td>33</td>\n",
       "      <td>0.812964</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7495</td>\n",
       "      <td>0.898505</td>\n",
       "      <td>34</td>\n",
       "      <td>0.824853</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7496</td>\n",
       "      <td>0.897284</td>\n",
       "      <td>32</td>\n",
       "      <td>0.849102</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7497</td>\n",
       "      <td>0.893106</td>\n",
       "      <td>33</td>\n",
       "      <td>0.812605</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7498</td>\n",
       "      <td>0.871964</td>\n",
       "      <td>33</td>\n",
       "      <td>0.779047</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video  short-term_memorability  nb_short-term_annotations  \\\n",
       "0   7494                 0.884959                         33   \n",
       "1   7495                 0.898505                         34   \n",
       "2   7496                 0.897284                         32   \n",
       "3   7497                 0.893106                         33   \n",
       "4   7498                 0.871964                         33   \n",
       "\n",
       "   long-term_memorability  nb_long-term_annotations  \n",
       "0                0.812964                        12  \n",
       "1                0.824853                        10  \n",
       "2                0.849102                        13  \n",
       "3                0.812605                        10  \n",
       "4                0.779047                        10  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sa_new=pd.DataFrame()\n",
    "pred_sa_new['video']=labels1['video']\n",
    "pred_sa_new['short-term_memorability'] = prediction_finalsa[:,0]\n",
    "pred_sa_new['nb_short-term_annotations']=labels1['nb_short-term_annotations']\n",
    "pred_sa_new['long-term_memorability'] = prediction_finalsa[:,1]\n",
    "pred_sa_new['nb_long-term_annotations']=labels1['nb_long-term_annotations']\n",
    "pred_sa_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the results for Simple Average\n",
    "pred_sa_new.to_csv(\"Results_Simple Average.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
